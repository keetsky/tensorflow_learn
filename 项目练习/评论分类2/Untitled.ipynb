{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据格式\n",
    "需要提取第一列和最后一列\n",
    "*  0– the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "*  1– the id of the tweet (2087)\n",
    "*  2– the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "*  3– the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "*  4– the user that tweeted (robotickilldozr)\n",
    "*  5– the text of the tweet (Lyx is cool)\n",
    "\n",
    "training.1600000.processed.noemoticon.csv（238M）\n",
    "testdata.manual.2009.06.14.csv（74K）\n",
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "   \n",
    "import pickle  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from collections import OrderedDict  \n",
    "   \n",
    "org_train_file = 'training.1600000.processed.noemoticon.csv'  \n",
    "org_test_file = 'testdata.manual.2009.06.14.csv'  \n",
    "   \n",
    "# 提取文件中有用的字段  \n",
    "def usefull_filed(org_file, output_file):  \n",
    "    output = open(output_file, 'w')  \n",
    "    with open(org_file, buffering=10000, encoding='latin-1') as f:  \n",
    "        try:  \n",
    "            for line in f:                # \"4\",\"2193601966\",\"Tue Jun 16 08:40:49 PDT 2009\",\"NO_QUERY\",\"AmandaMarie1028\",\"Just woke up. Having no school is the best feeling ever \"  \n",
    "                line = line.replace('\"', '')  \n",
    "                clf = line.split(',')[0]   # 4  \n",
    "                if clf == '0':  \n",
    "                    clf = [0, 0, 1]  # 消极评论  \n",
    "                elif clf == '2':  \n",
    "                    clf = [0, 1, 0]  # 中性评论  \n",
    "                elif clf == '4':  \n",
    "                    clf = [1, 0, 0]  # 积极评论  \n",
    "   \n",
    "                tweet = line.split(',')[-1]  \n",
    "                outputline = str(clf) + ':%:%:%:' + tweet  \n",
    "                output.write(outputline)  # [0, 0, 1]:%:%:%: that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
    "        except Exception as e:  \n",
    "            print(e)  \n",
    "    output.close()  # 处理完成，处理后文件大小127.5M  \n",
    "   \n",
    "usefull_filed(org_train_file, 'training.csv')  \n",
    "usefull_filed(org_test_file, 'tesing.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-94821c86d4fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mlex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lexcion.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-94821c86d4fe>\u001b[0m in \u001b[0;36mcreate_lexicon\u001b[0;34m(train_file)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':%:%:%:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m#对文字部分进行处理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/keetsky/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    110\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/keetsky/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/keetsky/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/keetsky/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/keetsky/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/keetsky/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/keetsky/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_lexicon(train_file):  \n",
    "    lex = []  \n",
    "    lemmatizer = WordNetLemmatizer()  \n",
    "    with open(train_file, buffering=10000, encoding='latin-1') as f:  \n",
    "        try:  \n",
    "            count_word = {}  # 统计单词出现次数  \n",
    "            for line in f:  \n",
    "                tweet = line.split(':%:%:%:')[1]  #对文字部分进行处理\n",
    "                words = word_tokenize(line.lower())  \n",
    "                for word in words:  \n",
    "                    word = lemmatizer.lemmatize(word)  \n",
    "                    if word not in count_word:  \n",
    "                        count_word[word] = 1  \n",
    "                    else:  \n",
    "                        count_word[word] += 1  \n",
    "   \n",
    "            count_word = OrderedDict(sorted(count_word.items(), key=lambda t: t[1]))  \n",
    "            for word in count_word:  \n",
    "                if count_word[word] < 100000 and count_word[word] > 100:  # 过滤掉一些词  \n",
    "                    lex.append(word)  \n",
    "        except Exception as e:  \n",
    "            print(e)  \n",
    "    return lex  \n",
    "   \n",
    "lex = create_lexicon('training.csv') \n",
    "with open('lexcion.pickle', 'wb') as f:  \n",
    "    pickle.dump(lex, f)  \n",
    "   \n",
    "   \n",
    "# 把字符串转为向量 \n",
    "def string_to_vector(input_file, output_file, lex): \n",
    "    output_f = open(output_file, 'w') \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    with open(input_file, buffering=10000, encoding='latin-1') as f: \n",
    "        for line in f: \n",
    "            label = line.split(':%:%:%:')[0] \n",
    "            tweet = line.split(':%:%:%:')[1] \n",
    "            words = word_tokenize(tweet.lower()) \n",
    "            words = [lemmatizer.lemmatize(word) for word in words] \n",
    "  \n",
    "            features = np.zeros(len(lex)) \n",
    "            for word in words: \n",
    "                if word in lex: \n",
    "                    features[lex.index(word)] = 1  # 一个句子中某个词可能出现两次,可以用+=1，其实区别不大 \n",
    "             \n",
    "            features = list(features) \n",
    "            output_f.write(str(label) + \":\" + str(features) + '\\n') \n",
    "    output_f.close() \n",
    "  \n",
    "  \n",
    "f = open('lexcion.pickle', 'rb') \n",
    "lex = pickle.load(f) \n",
    "f.close() \n",
    "  \n",
    "# lexcion词汇表大小112k,training.vec大约112k*1600000  170G  太大，只能边转边训练了 \n",
    "# string_to_vector('training.csv', 'training.vec', lex) \n",
    "# string_to_vector('tesing.csv', 'tesing.vec', lex)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lexcion.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ef099e49305a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lexcion.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lexcion.pickle'"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import random   \n",
    "import tensorflow as tf  \n",
    "import pickle  \n",
    "import numpy as np  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "   \n",
    "f = open('lexcion.pickle', 'rb')  \n",
    "lex = pickle.load(f)  \n",
    "f.close()  \n",
    "   \n",
    "   \n",
    "def get_random_line(file, point):  \n",
    "    file.seek(point)  \n",
    "    file.readline()  \n",
    "    return file.readline()  \n",
    "# 从文件中随机选择n条记录  \n",
    "def get_n_random_line(file_name, n=150):  \n",
    "    lines = []  \n",
    "    file = open(file_name, encoding='latin-1')  \n",
    "    total_bytes = os.stat(file_name).st_size   \n",
    "    for i in range(n):  \n",
    "        random_point = random.randint(0, total_bytes)  \n",
    "        lines.append(get_random_line(file, random_point))  \n",
    "    file.close()  \n",
    "    return lines  \n",
    "   \n",
    "   \n",
    "def get_test_dataset(test_file):  \n",
    "    with open(test_file, encoding='latin-1') as f:  \n",
    "        test_x = []  \n",
    "        test_y = []  \n",
    "        lemmatizer = WordNetLemmatizer()  \n",
    "        for line in f:  \n",
    "            label = line.split(':%:%:%:')[0]  \n",
    "            tweet = line.split(':%:%:%:')[1]  \n",
    "            words = word_tokenize(tweet.lower())  \n",
    "            words = [lemmatizer.lemmatize(word) for word in words]  \n",
    "            features = np.zeros(len(lex))  \n",
    "            for word in words:  \n",
    "                if word in lex:  \n",
    "                    features[lex.index(word)] = 1  \n",
    "              \n",
    "            test_x.append(list(features))  \n",
    "            test_y.append(eval(label))  \n",
    "    return test_x, test_y  \n",
    "   \n",
    "test_x, test_y = get_test_dataset('tesing.csv')  \n",
    "   \n",
    "   \n",
    "#######################################################################  \n",
    "   \n",
    "n_input_layer = len(lex)  # 输入层  \n",
    "   \n",
    "n_layer_1 = 2000     # hide layer  \n",
    "n_layer_2 = 2000    # hide layer(隐藏层)听着很神秘，其实就是除输入输出层外的中间层  \n",
    "   \n",
    "n_output_layer = 3       # 输出层  \n",
    "   \n",
    "   \n",
    "def neural_network(data):  \n",
    "    # 定义第一层\"神经元\"的权重和biases  \n",
    "    layer_1_w_b = {'w_':tf.Variable(tf.random_normal([n_input_layer, n_layer_1])), 'b_':tf.Variable(tf.random_normal([n_layer_1]))}  \n",
    "    # 定义第二层\"神经元\"的权重和biases  \n",
    "    layer_2_w_b = {'w_':tf.Variable(tf.random_normal([n_layer_1, n_layer_2])), 'b_':tf.Variable(tf.random_normal([n_layer_2]))}  \n",
    "    # 定义输出层\"神经元\"的权重和biases  \n",
    "    layer_output_w_b = {'w_':tf.Variable(tf.random_normal([n_layer_2, n_output_layer])), 'b_':tf.Variable(tf.random_normal([n_output_layer]))}  \n",
    "   \n",
    "    # w·x+b  \n",
    "    layer_1 = tf.add(tf.matmul(data, layer_1_w_b['w_']), layer_1_w_b['b_'])  \n",
    "    layer_1 = tf.nn.relu(layer_1)  # 激活函数  \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, layer_2_w_b['w_']), layer_2_w_b['b_'])  \n",
    "    layer_2 = tf.nn.relu(layer_2 ) # 激活函数  \n",
    "    layer_output = tf.add(tf.matmul(layer_2, layer_output_w_b['w_']), layer_output_w_b['b_'])  \n",
    "   \n",
    "    return layer_output  \n",
    "   \n",
    "   \n",
    "X = tf.placeholder('float')  \n",
    "Y = tf.placeholder('float')  \n",
    "batch_size = 90  \n",
    "   \n",
    "def train_neural_network(X, Y):  \n",
    "    predict = neural_network(X)  \n",
    "    cost_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(predict, Y))  \n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost_func)  \n",
    "   \n",
    "    with tf.Session() as session:  \n",
    "        session.run(tf.initialize_all_variables())  \n",
    "   \n",
    "        lemmatizer = WordNetLemmatizer()  \n",
    "        saver = tf.train.Saver()  \n",
    "        i = 0  \n",
    "        pre_accuracy = 0  \n",
    "        while True:   # 一直训练  \n",
    "            batch_x = []  \n",
    "            batch_y = []  \n",
    "   \n",
    "            #if model.ckpt文件已存在:  \n",
    "            #   saver.restore(session, 'model.ckpt')  恢复保存的session  \n",
    "   \n",
    "            try:  \n",
    "                lines = get_n_random_line('training.csv', batch_size)  \n",
    "                for line in lines:  \n",
    "                    label = line.split(':%:%:%:')[0]  \n",
    "                    tweet = line.split(':%:%:%:')[1]  \n",
    "                    words = word_tokenize(tweet.lower())  \n",
    "                    words = [lemmatizer.lemmatize(word) for word in words]  \n",
    "   \n",
    "                    features = np.zeros(len(lex))  \n",
    "                    for word in words:  \n",
    "                        if word in lex:  \n",
    "                            features[lex.index(word)] = 1  # 一个句子中某个词可能出现两次,可以用+=1，其实区别不大  \n",
    "                  \n",
    "                    batch_x.append(list(features))  \n",
    "                    batch_y.append(eval(label))  \n",
    "   \n",
    "                session.run([optimizer, cost_func], feed_dict={X:batch_x,Y:batch_y})  \n",
    "            except Exception as e:  \n",
    "                print(e)  \n",
    "   \n",
    "            # 准确率  \n",
    "            if i > 100:  \n",
    "                correct = tf.equal(tf.argmax(predict,1), tf.argmax(Y,1))  \n",
    "                accuracy = tf.reduce_mean(tf.cast(correct,'float'))  \n",
    "                accuracy = accuracy.eval({X:test_x, Y:test_y})  \n",
    "                if accuracy > pre_accuracy:  # 保存准确率最高的训练模型  \n",
    "                    print('准确率: ', accuracy)  \n",
    "                    pre_accuracy = accuracy  \n",
    "                    saver.save(session, 'model.ckpt')  # 保存session  \n",
    "                i = 0  \n",
    "            i += 1  \n",
    "   \n",
    "   \n",
    "train_neural_network(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
